# ğŸ¤ Vocalix â€” Conversational AI Voice Agent

Imagine chatting with a super-smart friend who listens, understands, and replies in a natural voice. It even remembers past conversations â€” mention your dog once, and it might ask about them later. I designed it to feel like talking to a person, not just using an app.

_Part of my_ **#30DaysOfVoiceAgents** _challenge â€” building an end-to-end AI-powered voice assistant that feels natural, responsive, and production-ready._

---

## ğŸ“Œ Overview

Vocalix is a **dual-mode Voice AI application** designed for real-time, human-like interactions.

- **Voice Agent Assistant (Text-to-Speech)** â€” Type anything, choose a voice, and instantly hear it in a lifelike tone.
- **Conversational Voice Bot (LLM + TTS)** â€” Speak naturally, let the AI transcribe, think, and respond in a natural-sounding voice.

Itâ€™s built for **speed**, **clarity**, and **engagement** â€” from the **glassmorphic UI** to the **seamless speech pipeline**.

---

## âœ¨ Features

- ğŸ™ **Single Smart Record Button** â€” Start/Stop recording with one tap, animated for intuitive feedback.
- â± **Live Recording Timer** â€” Shows how long youâ€™ve been speaking.
- ğŸ”Š **Auto-Playback** â€” AI responses play instantly without manual clicks.
- ğŸ§  **Full AI Pipeline**:
  1. ğŸ¤ **Voice Input** â€” Browser Recording API
  2. âœ **AssemblyAI** â€” Real-time transcription (Speech-to-Text)
  3. ğŸ’¬ **Google Gemini** â€” Conversational reasoning (LLM)
  4. ğŸ”‰ **Murf AI** â€” Human-like speech generation (Text-to-Speech)
- ğŸ’¾ **Chat History Memory** â€” Maintains session-wise context between user and AI.

---

## ğŸ—“ï¸ Day 1 â€“ Kickoff

ğŸš€ Started the Murf AI Voice Agent Challenge
ğŸ’» Set up FastAPI project & tested Murf API
ğŸ‰ Generated first TTS audio successfully

## ğŸ—“ï¸ Day 2 â€“ TTS Integration

ğŸ”— Linked Murf TTS API with FastAPI
ğŸ§ Built UI for text input & playback
ğŸ› ï¸ Completed end-to-end TTS with error handling

## ğŸ—“ï¸ Day 3 â€“ Voice Agent UX

ğŸ–Œï¸ Polished the UI with improved design (HTML/CSS)
ğŸ”„ Refactored API flow for smoother UX
ğŸ’¡ Learned how to make the voice interaction feel more natural
ğŸ™Œ Thanked Murf AI publicly for enabling student creativity

## ğŸ—“ï¸ Day 4 â€“ Echo Bot ğŸ¤

ğŸª Added Echo Bot UI section
ğŸ™ï¸ Used MediaRecorder API to record & play audio
ğŸ§  Learned real-time browser audio handling
âœ¨ Built foundation for future speech input

## ğŸ—“ï¸ Day 5 â€“ Audio Upload & Integration â˜ï¸

âºï¸ Enabled Echo Bot to send recordings to FastAPI
ğŸ› ï¸ Added /upload API to save audio & return details
ğŸ”” UI now shows live upload status
ğŸ”ƒ Achieved smooth mic â†’ server â†’ playback flow

## ğŸ—“ï¸ Day 6 â€“ Transcription âœï¸

ğŸ§µ Added /transcribe/file endpoint with AssemblyAI
ğŸ“¤ Audio upload now returns text
ğŸ–¥ï¸ Integrated transcription display in UI
ğŸ“œ Complete flow: record â†’ upload â†’ transcribe â†’ show text

## ğŸ—“ï¸ Day 7 â€“ Voice-to-Voice ğŸ¤ğŸ”„ğŸ™ï¸

ğŸ†• Added /tts/echo endpoint
ğŸ™ï¸ Flow: record â†’ transcribe (AssemblyAI) â†’ Murf TTS â†’ return audio URL
ğŸ”„ Complete voice-to-voice pipeline now live

## ğŸ—“ï¸ Day 8 â€“ LLM Query Endpoint ğŸ§ ğŸ’¬

ğŸ†• Added /llm/query in FastAPI
ğŸ“© Sends text to Google Gemini & returns AI reply
âš¡ Using gemini-2.5-pro for fast responses
ğŸ› ï¸ Added helper & error handling
ğŸ’¡ First step toward natural conversational AI

## ğŸ—“ï¸ Day 9 â€“ Audio-to-Audio AI Chat ğŸ¤ğŸ¤–ğŸ™ï¸

ğŸ”„ /llm/query now accepts audio from browser
ğŸ“‹ Flow: record â†’ transcribe (AssemblyAI) â†’ Gemini reply â†’ Murf TTS â†’ return audio
ğŸ§ AI responds instantly with lifelike voice
âœ¨ No text input needed â€” full voice conversation

## ğŸ—“ï¸ Day 10 â€“ Chat Memory ğŸ—‚ï¸ğŸ—£ï¸

ğŸ§  Added context so AI remembers past messages
ğŸ†• /agent/chat/{session_id} handles audio with session history
ğŸ“‹ Flow: record â†’ transcribe â†’ store in memory â†’ Gemini reply â†’ Murf TTS â†’ return audio
ğŸ¯ Enables smooth, context-aware voice conversations

## ğŸ—“ï¸ Day 11 â€“ Robust Error Handling ğŸ›¡ï¸âš™ï¸

ğŸ”’ Wrapped STT (AssemblyAI), LLM (Gemini), and TTS (Murf) calls in try/except blocks
ğŸ†• Added clear error messages in the UI
ğŸ”Š Implemented fallback audio responses so the bot still â€œspeaksâ€ even when an API fails
ğŸ› ï¸ Simulated API failures by disabling API keys to test resilience
ğŸ–Œï¸ Styled error messages for better visibility in the chat

## ğŸ—“ï¸ Day 12 â€“ Conversational Agent UI Revamp ğŸ¨ğŸ–¥ï¸

Gave my AI Voice Agent a fresh, modern look:
âœ¨ Merged â€œStart Recordingâ€ + â€œStop Recordingâ€ into one smart toggle button
âœ¨ Hidden the audio player â€” audio now plays automatically after loading
âœ¨ Gradient theme & transparent container
âœ¨ Animated, prominent record button
âœ¨ Clean, focused conversational UI

---

## ğŸ›  Tech Stack

**Frontend:** HTML, CSS, JavaScript (Vanilla JS)  
**Backend:** Python (FastAPI)  
**Speech-to-Text:** [AssemblyAI](https://www.assemblyai.com/)  
**LLM:** [Google Gemini API](https://ai.google.dev/)  
**Text-to-Speech:** [Murf AI](https://murf.ai/)  
**Runtime:** Python 3.9+  
**Hosting:** Local / Any cloud platform (optional)

---

## ğŸ— Architecture

```mermaid
flowchart TD
    A[ğŸ™ User Speaks] -->|Audio| B[FastAPI Server]
    B --> C[AssemblyAI STT]
    C --> D[Google Gemini LLM]
    D --> E[Murf AI TTS]
    E -->|Audio| F[ğŸ§ User Hears AI Response]
    B --> G[ğŸ’¾ Chat History Store]
```

---

# ğŸ›  Installation & Run Instructions

## ğŸ“‚ Project Structure

```
MURF-AI-AGENT/
â”‚
â”œâ”€â”€ main.py                     # Main FastAPI application
â”œâ”€â”€ schemas.py                  # Pydantic models for request/response
â”œâ”€â”€ debug_utils.py             # Debugging utilities
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ README.md                 # Project documentation
â”‚
â”œâ”€â”€ services/                 # Business logic services
â”‚   â”œâ”€â”€ __init__.py          # Service exports
â”‚   â”œâ”€â”€ stt_service.py       # Speech-to-Text service (AssemblyAI)
â”‚   â”œâ”€â”€ tts_service.py       # Text-to-Speech service (Murf)
â”‚   â”œâ”€â”€ llm_service.py       # Large Language Model service (Gemini)
â”‚   â””â”€â”€ chat_service.py      # Chat session management service
â”‚
â”œâ”€â”€ static/                  # Static files
â”‚   â”œâ”€â”€ favicon_io/
â”‚   â”‚   â””â”€â”€ favicon.ico
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ script.js
â”‚
â””â”€â”€ templates/               # HTML templates
    â””â”€â”€ index.html          # Main web interface
```

## âš™ï¸ Environment Variables

Create a .env file in the root directory:

```
ASSEMBLYAI_API_KEY=your_assemblyai_api_key
GEMINI_API_KEY=your_gemini_api_key
MURF_API_KEY=your_murf_api_key
```

## ğŸš€ How to Run

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/jaikh316/vocalix-voice-agent.git
cd vocalix-voice-agent
```

### 2ï¸âƒ£ Set Up Python Environment

```
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Environment Variables

Create .env file as described above.

### 5ï¸âƒ£ Run the Server

```
uvicorn main:app --reload
```

### 6ï¸âƒ£ Open in Browser

```
http://127.0.0.1:8000
```

---

## ğŸ™Œ Acknowledgements

**AssemblyAI** â€” STT

**Google Gemini** â€” LLM

**Murf AI** â€” TTS

**FastAPI** â€” Backend Framework

---
